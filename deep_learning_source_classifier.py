# -*- coding: utf-8 -*-
"""Deep Learning - Source Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UupJLEpYeYj0d63-1jGEUCArho6FROwG
"""

#Import file from Google Drive

from google.colab import drive

drive.mount('/content/drive/')

#Drive Location
#!ls '/content/drive/My Drive/CPS 580 Project File/Species Classifier Dataset/train_X.pkl'
#!ls '/content/drive/My Drive/CPS 580 Project File/Species Classifier Dataset/test_X.pkl'
#!ls '/content/drive/My Drive/CPS 580 Project File/Species Classifier Dataset/train_Y.pkl'
#!ls '/content/drive/My Drive/CPS 580 Project File/Species Classifier Dataset/test_Y.pkl'

#Set the data values to train_X, test_X, val_X, train_Y, test_Y, and val_Y
import pickle

#Unpickle a Python File and Put it onto data variable
with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_X.pkl', 'rb') as X1:
  train_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_X.pkl', 'rb') as X2:
  test_X = pickle.load(X2, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_Y.pkl', 'rb') as Y1:
  train_Y = pickle.load(Y1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_Y.pkl', 'rb') as Y2:
  test_Y = pickle.load(Y2, encoding = 'latin1')

#Double Check if the shapes of the datasets are what you saved as
import numpy as np

train_X = train_X[:1600, ]
train_Y = train_Y[:1600, ]
val_X = train_X[-400:,]
val_Y = train_Y[-400:,]

print("Shape of train_X =", np.array(train_X).shape)
print("Shape of train_Y =", np.array(train_Y).shape)

print("Shape of test_X =", np.array(test_X).shape)
print("Shape of test_Y =", np.array(test_Y).shape)

print("Shape of val_X =", np.array(val_X).shape)
print("Shape of val_Y =", np.array(val_Y).shape)

#Normalize train_X, test_X, val_X
train_X = train_X / 255
test_X = test_X / 255

"""Build Model as a Classifier"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Convolutional layers
cnn_model = Sequential()
cnn_model.add(Conv2D(16, (3, 3), activation='selu', input_shape=(75, 200, 3)))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='selu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='selu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='selu'))
cnn_model.add(MaxPooling2D((2, 2)))

# Dense, fully connected layers
cnn_model.add(Flatten())
cnn_model.add(Dense(16, activation='selu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, activation='selu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation='softmax'))

# Specify the loss, optimizer and any additional metrics to follow
#opt = optimizers.Adam(learning_rate = 0.0)
cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])

cnn_model.summary()
# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 50, batch_size = 50, validation_data= (val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (Naive Predictor)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""K-fold Validation"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_X.pkl', 'rb') as X1:
  train_data = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_Y.pkl', 'rb') as Y1:
  train_targets = pickle.load(Y1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_X.pkl', 'rb') as X2:
  test_X = pickle.load(X2, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_Y.pkl', 'rb') as Y2:
  test_Y = pickle.load(Y2, encoding = 'latin1')

k = 5
num_val_samples = len(train_data) // k
num_epochs = 50
all_scores = []

print("Shape of train_X =", np.array(train_data).shape)
print("Shape of train_Y =", np.array(train_targets).shape)

#Normalize train_X, test_X, val_X
train_data = train_data / 255.

# Convolutional layers
def build_model():
  cnn_model = Sequential()
  cnn_model.add(Conv2D(256, (3, 3), activation='elu', input_shape=(75, 200, 3)))
  cnn_model.add(MaxPooling2D((2, 2)))

  cnn_model.add(Conv2D(128, (3, 3), activation='elu'))
  cnn_model.add(MaxPooling2D((2, 2)))

  cnn_model.add(Conv2D(64, (3, 3), activation='elu'))
  cnn_model.add(MaxPooling2D((2, 2)))

  # Dense, fully connected layers
  cnn_model.add(Flatten())
  cnn_model.add(Dense(64, activation='elu'))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(16, activation='elu'))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(4, activation='softmax'))
  cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])

  return cnn_model

all_loss_hist = []
all_acc_hist = []
all_valloss_hist = []
all_valacc_hist = []

for i in range(k):
  print("Processing Fold #:", i)

  val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]

  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],
       train_data[(i+1) * num_val_samples:]], axis = 0
  )

  partial_train_targets = np.concatenate(
      [train_targets[:i * num_val_samples],
       train_targets[(i+1) * num_val_samples:]], axis = 0
  )
  
  #Augment the Training Data
  #datagen = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1)
  #batch_size = 50

  cnn_model = build_model()
  
  # Train the model with the training data, set epochs and batch size
  history = cnn_model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 20, validation_data= (val_data, val_targets), verbose = 0)
  #val_loss, val_acc = cnn_model.evaluate(val_data, val_targets)
  #all_scores.append(val_acc)
  cnn_model.evaluate(test_X, test_Y)
  
  loss_hist = history.history['loss']
  acc_hist = history.history['accuracy']
  val_loss_hist = history.history['val_loss']
  val_acc_hist = history.history['val_accuracy']

  all_loss_hist.append(loss_hist)
  all_acc_hist.append(acc_hist)
  all_valloss_hist.append(val_loss_hist)
  all_valacc_hist.append(val_acc_hist)


#all_scores = np.array(all_scores)
#print(np.mean(all_scores))

avg_val_loss = [np.mean([x[i] for x in all_valloss_hist]) for i in range(num_epochs)]
avg_val_acc = [np.mean([x[i] for x in all_valacc_hist]) for i in range(num_epochs)]
avg_acc = [np.mean([x[i] for x in all_acc_hist]) for i in range(num_epochs)]
avg_loss = [np.mean([x[i] for x in all_loss_hist]) for i in range(num_epochs)]

#Plotting training and validation loss
import matplotlib.pyplot as plt

epochs = range(1, len(avg_loss) + 1)

plt.plot(epochs, avg_loss, 'b', label = 'Training loss')
plt.plot(epochs, avg_val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt


epochs = range(1, len(avg_acc) + 1)

plt.plot(epochs, avg_acc, 'b', label = 'Training accuracy')
plt.plot(epochs, avg_val_acc, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Data Augmentation"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_X.pkl', 'rb') as X1:
  train_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_X.pkl', 'rb') as X2:
  test_X = pickle.load(X2, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_Y.pkl', 'rb') as Y1:
  train_Y = pickle.load(Y1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_Y.pkl', 'rb') as Y2:
  test_Y = pickle.load(Y2, encoding = 'latin1')

train_X = train_X[:1600, ]
train_Y = train_Y[:1600, ]
val_X = train_X[-400:,]
val_Y = train_Y[-400:,]

#Normalize train_X, test_X, val_X
train_X = train_X / 255
test_X = test_X / 255

#Build a Model
cnn_model = Sequential()
cnn_model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(75, 200, 3)))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

# Dense, fully connected layers
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dense(4, activation='softmax'))
cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])

#Augment the Training Data
datagen = ImageDataGenerator(rotation_range = 5, width_shift_range = 0.01, height_shift_range = 0.01)
batch_size = 50

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(datagen.flow(train_X, train_Y, batch_size = batch_size), steps_per_epoch = len(train_X)/batch_size, epochs = 100, validation_data = (val_X, val_Y))

#Test it on the Test Data
cnn_model.evaluate(test_X, test_Y)

"""Try VGG16"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import keras
from keras import backend, optimizers
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import *
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools
import pickle
from keras import utils, regularizers, optimizers
# %matplotlib inline

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_X.pkl', 'rb') as X1:
  train_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_X.pkl', 'rb') as X2:
  test_X = pickle.load(X2, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_Y.pkl', 'rb') as Y1:
  train_Y = pickle.load(Y1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_Y.pkl', 'rb') as Y2:
  test_Y = pickle.load(Y2, encoding = 'latin1')

train_X = train_X[:1600, ]
train_Y = train_Y[:1600, ]
val_X = train_X[-400:,]
val_Y = train_Y[-400:,]

train_X = train_X / 255.
test_X = test_X / 255.

"""Hyperparameter Testing 1"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation = 'softmax'))
cnn_model.summary()

optimizer = keras.optimizers.Nadam(lr = 0.0001)
cnn_model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 30, batch_size = 10, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss (Best Model)")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (Best Model)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Testing 2"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation = 'softmax'))
cnn_model.summary()

#optimizer = keras.optimizers.Nadam(lr = 0.0001)
cnn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 50, batch_size = 15, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss (HypTest2)")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (HypTest2)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Testing 3"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(128, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(128, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation = 'softmax'))
cnn_model.summary()

#optimizer = keras.optimizers.Nadam(lr = 0.0001)
cnn_model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 30, batch_size = 10, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss (HypTest3)")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (HypTest3)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Testing 4"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation = 'softmax'))
cnn_model.summary()

optimizer = keras.optimizers.Nadam(lr = 0.0001)
cnn_model.compile(optimizer = 'nadam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 50, batch_size = 16, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss (HypTest4)")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (HypTest4)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparaemeter Testing 5"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(4, activation = 'softmax'))
cnn_model.summary()

optimizer = keras.optimizers.Nadam(lr = 0.0001)
cnn_model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 30, batch_size = 10, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss (HypTest4)")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy (HypTest4)")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""VGG16 with K-fold validation"""

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras
from keras import backend
from keras.models import Sequential
from keras.layers.convolutional import *
import pickle
import itertools
# %matplotlib inline

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_X.pkl', 'rb') as X1:
  train_data = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_X.pkl', 'rb') as X2:
  test_data = pickle.load(X2, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/train_Y.pkl', 'rb') as Y1:
  train_targets = pickle.load(Y1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Source Classifier Dataset/test_Y.pkl', 'rb') as Y2:
  test_Y = pickle.load(Y2, encoding = 'latin1')

train_data = train_data / 255.
test_data = test_data / 255.

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

k = 5
num_val_samples = len(train_data) // k
num_epochs = 80
all_scores = []

print("Shape of train_X =", np.array(train_data).shape)
print("Shape of train_Y =", np.array(train_targets).shape)

#Normalize train_X, test_X, val_X

# Convolutional layers
def build_model():
  cnn_model = Sequential()
  for layer in vgg16_model.layers:
    cnn_model.add(layer)

  for layer in cnn_model.layers:
    layer.trainable = False

  cnn_model.add(Flatten())
  cnn_model.add(Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(32, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(4, activation = 'softmax'))

  return cnn_model

all_loss_hist = []
all_acc_hist = []
all_valloss_hist = []
all_valacc_hist = []

#datagen = ImageDataGenerator(rotation_range = 5, width_shift_range = 0.01, height_shift_range = 0.01)
#batch_size = 40

for i in range(k):
  print("Processing Fold #:", i)

  val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]

  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],
       train_data[(i+1) * num_val_samples:]], axis = 0
  )

  partial_train_targets = np.concatenate(
      [train_targets[:i * num_val_samples],
       train_targets[(i+1) * num_val_samples:]], axis = 0
  )

  cnn_model = build_model()
  
  # Train the model with the training data, set epochs and batch size
  cnn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

  # Train the model with the training data, set epochs and batch size
  history = cnn_model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 15, validation_data=(val_data, val_targets), verbose = 0)

  cnn_model.evaluate(test_X, test_Y)

  loss_hist = history.history['loss']
  acc_hist = history.history['accuracy']
  val_loss_hist = history.history['val_loss']
  val_acc_hist = history.history['val_accuracy']

  all_loss_hist.append(loss_hist)
  all_acc_hist.append(acc_hist)
  all_valloss_hist.append(val_loss_hist)
  all_valacc_hist.append(val_acc_hist)

avg_val_loss = [np.mean([x[i] for x in all_valloss_hist]) for i in range(num_epochs)]
avg_val_acc = [np.mean([x[i] for x in all_valacc_hist]) for i in range(num_epochs)]
avg_acc = [np.mean([x[i] for x in all_acc_hist]) for i in range(num_epochs)]
avg_loss = [np.mean([x[i] for x in all_loss_hist]) for i in range(num_epochs)]

#Plotting training and validation loss
import matplotlib.pyplot as plt

print(history_dict.keys())

epochs = range(1, len(avg_loss) + 1)

plt.plot(epochs, avg_loss, 'b', label = 'Training loss')
plt.plot(epochs, avg_val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
epochs = range(1, len(avg_acc) + 1)

plt.plot(epochs, avg_acc, 'b', label = 'Training accuracy')
plt.plot(epochs, avg_val_acc, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()