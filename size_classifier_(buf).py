# -*- coding: utf-8 -*-
"""Size Classifier (BUF)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z5RRUmgckOkZKnfbmS0yr1ZAKg3poMsD
"""

#Import file from Google Drive

from google.colab import drive

drive.mount('/content/drive/')

#Set the data values to train_X, test_X, val_X, train_Y, test_Y, and val_Y
import pickle

#Unpickle a Python File and Put it onto data variable
#BUF
with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_X.pkl', 'rb') as X1:
  BUF_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_Y.pkl', 'rb') as X1:
  BUF_Y = pickle.load(X1, encoding = 'latin1')

print(BUF_X.shape)
print(BUF_Y.shape)

BUF_X = BUF_X / 255.

train_X = BUF_X[:200, ]
train_Y = BUF_Y[:200, ]
val_X = BUF_X[200:266,]
val_Y = BUF_Y[200:266,]
test_X = BUF_X[-67:,]
test_Y = BUF_Y[-67:,]

print(train_X.shape, train_Y.shape, val_X.shape, val_Y.shape, test_X.shape, test_Y.shape)

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Convolutional layers
cnn_model = Sequential()
cnn_model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(75, 200, 3)))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(16, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

# Dense, fully connected layers
cnn_model.add(Flatten())
cnn_model.add(Dense(16, activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(8, activation='elu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation='softmax'))

# Specify the loss, optimizer and any additional metrics to follow
cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])
cnn_model.summary()

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 50, batch_size = 20, validation_data= (val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""With K-fold validation"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_X.pkl', 'rb') as X1:
  BUF_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_Y.pkl', 'rb') as X1:
  BUF_Y = pickle.load(X1, encoding = 'latin1')

BUF_X = BUF_X/255.

train_data = BUF_X[:266, ]
train_targets = BUF_Y[:266, ]
test_X = BUF_X[-67:,]
test_Y = BUF_Y[-67:,]
print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)

k = 5
num_val_samples = len(train_X) // k
num_epochs = 30
all_scores = []

print("Shape of train_X =", np.array(train_data).shape)
print("Shape of train_Y =", np.array(train_targets).shape)

#Normalize train_X, test_X, val_X

# Convolutional layers
def build_model():
  cnn_model = Sequential()
  cnn_model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(75, 200, 3)))
  cnn_model.add(MaxPooling2D((2, 2)))

  cnn_model.add(Conv2D(16, (3, 3), activation='elu'))
  cnn_model.add(MaxPooling2D((2, 2)))

  cnn_model.add(Conv2D(8, (3, 3), activation='elu'))
  cnn_model.add(MaxPooling2D((2, 2)))

    # Dense, fully connected layers
  cnn_model.add(Flatten())
  cnn_model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
  #cnn_model.add(Dropout(0.5))
  #cnn_model.add(Dense(16, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
  #cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(3, activation='softmax'))
  cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])

  return cnn_model

all_loss_hist = []
all_acc_hist = []
all_valloss_hist = []
all_valacc_hist = []

for i in range(k):
  print("Processing Fold #:", i)

  val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]

  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],
       train_data[(i+1) * num_val_samples:]], axis = 0
  )

  partial_train_targets = np.concatenate(
      [train_targets[:i * num_val_samples],
       train_targets[(i+1) * num_val_samples:]], axis = 0
  )

  cnn_model = build_model()
  
  # Train the model with the training data, set epochs and batch size
  #cnn_model.fit(datagen.flow(partial_train_data, partial_train_targets, batch_size = batch_size), steps_per_epoch = len(partial_train_data)/batch_size, epochs = num_epochs, validation_data = (val_X, val_Y))
  history = cnn_model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 10, validation_data= (val_data, val_targets), verbose = 0)
  #val_loss, val_acc = cnn_model.evaluate(val_data, val_targets)
  #all_scores.append(val_acc)
  cnn_model.evaluate(test_X, test_Y)
  loss_hist = history.history['loss']
  acc_hist = history.history['accuracy']
  val_loss_hist = history.history['val_loss']
  val_acc_hist = history.history['val_accuracy']

  all_loss_hist.append(loss_hist)
  all_acc_hist.append(acc_hist)
  all_valloss_hist.append(val_loss_hist)
  all_valacc_hist.append(val_acc_hist)

avg_val_loss = [np.mean([x[i] for x in all_valloss_hist]) for i in range(num_epochs)]
avg_val_acc = [np.mean([x[i] for x in all_valacc_hist]) for i in range(num_epochs)]
avg_acc = [np.mean([x[i] for x in all_acc_hist]) for i in range(num_epochs)]
avg_loss = [np.mean([x[i] for x in all_loss_hist]) for i in range(num_epochs)]

#Plotting training and validation loss
import matplotlib.pyplot as plt

print(history_dict.keys())

epochs = range(1, len(avg_loss) + 1)

plt.plot(epochs, avg_loss, 'b', label = 'Training loss')
plt.plot(epochs, avg_val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(1, len(avg_acc) + 1)

plt.plot(epochs, avg_acc, 'b', label = 'Training accuracy')
plt.plot(epochs, avg_val_acc, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Data Augmentation"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_X.pkl', 'rb') as X1:
  BUF_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_Y.pkl', 'rb') as X1:
  BUF_Y = pickle.load(X1, encoding = 'latin1')

BUF_X = BUF_X/255.

train_X = BUF_X[:200, ]
train_Y = BUF_Y[:200, ]
val_X = BUF_X[200:266,]
val_Y = BUF_Y[200:266,]
test_X = BUF_X[-67:,]
test_Y = BUF_Y[-67:,]

#Build a Model
cnn_model = Sequential()
cnn_model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(75, 200, 3)))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(16, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(8, (3, 3), activation='elu'))
cnn_model.add(MaxPooling2D((2, 2)))

# Dense, fully connected layers
cnn_model.add(Flatten())
cnn_model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(16, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation='softmax'))
cnn_model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])

#Augment the Training Data
datagen = ImageDataGenerator(rotation_range = 5, width_shift_range = 0.01, height_shift_range = 0.01, zoom_range = 0.2, shear_range = 0.2)
batch_size = 25

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(datagen.flow(train_X, train_Y, batch_size = batch_size), steps_per_epoch = len(train_X)/batch_size, epochs = 100, validation_data = (val_X, val_Y))

#Test it on the Test Data
cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""See VGG16"""

# Commented out IPython magic to ensure Python compatibility.
#Import all the libraries

import numpy as np
import keras
from keras import backend, optimizers
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import *
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools
from keras import utils, regularizers, optimizers
# %matplotlib inline
import pickle

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_X.pkl', 'rb') as X1:
  BUF_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_Y.pkl', 'rb') as X1:
  BUF_Y = pickle.load(X1, encoding = 'latin1')

train_X = BUF_X[:200, ]
train_Y = BUF_Y[:200, ]
val_X = BUF_X[200:266,]
val_Y = BUF_Y[200:266,]
test_X = BUF_X[-67:,]
test_Y = BUF_Y[-67:,]

train_X = train_X / 255

val_X = val_X / 255

test_X = test_X / 255

"""Hyperparameter Test 1"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

type(vgg16_model)

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

cnn_model.summary()
type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation = 'softmax'))
cnn_model.summary()

optimizer = keras.optimizers.Nadam(lr = 0.001)
cnn_model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 60, batch_size = 15, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Test 2"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation = 'softmax'))

optimizer = keras.optimizers.Nadam(lr = 0.001)
cnn_model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 70, batch_size = 20, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Test 3"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(32, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dense(3, activation = 'softmax'))

#optimizer = keras.optimizers.Nadam(lr = 0.001)
cnn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 60, batch_size = 45, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Test 4"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation = 'softmax'))

#optimizer = keras.optimizers.Nadam(lr = 0.001)
cnn_model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 55, batch_size = 15, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Hyperparameter Test 5"""

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

cnn_model = Sequential()
for layer in vgg16_model.layers:
  cnn_model.add(layer)

type(cnn_model)

for layer in cnn_model.layers:
  layer.trainable = False

cnn_model.add(Flatten())
cnn_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(128, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(32, activation='elu', kernel_regularizer = regularizers.l2(0.01)))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation = 'softmax'))

optimizer = keras.optimizers.Nadam(lr = 0.001)
cnn_model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Train the model with the training data, set epochs and batch size
history = cnn_model.fit(train_X, train_Y, epochs = 60, batch_size = 15, validation_data=(val_X, val_Y))

cnn_model.evaluate(test_X, test_Y)

history_dict = history.history
print(history_dict.keys())

import matplotlib.pyplot as plt

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

history_dict = history.history
print(history_dict.keys())

train_loss = history_dict['accuracy']
val_loss = history_dict['val_accuracy']

epochs = range(1, len(history_dict['accuracy']) + 1)
print(epochs)

plt.plot(epochs, train_loss, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_loss, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""VGG16 with K-fold validation"""

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.datasets import mnist
from keras import utils, regularizers, optimizers
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras
from keras import backend
from keras.models import Sequential
from keras.layers.convolutional import *
import itertools
# %matplotlib inline

#Generate Fine-Tuned VGG16 model

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_X.pkl', 'rb') as X1:
  BUF_X = pickle.load(X1, encoding = 'latin1')

with open('/content/drive/My Drive/CPS 580 Project File/Species Specific Size Classifier/BUF_Y.pkl', 'rb') as X1:
  BUF_Y = pickle.load(X1, encoding = 'latin1')

BUF_X = BUF_X / 255.

train_data = BUF_X[:266, ]
train_targets = BUF_Y[:266, ]
test_X = BUF_X[-67:,]
test_Y = BUF_Y[-67:,]
print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)

vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=(75, 200, 3), include_top=False)
vgg16_model.summary()

type(vgg16_model)

k = 5
num_val_samples = len(train_data) // k
num_epochs = 60
all_scores = []

print("Shape of train_X =", np.array(train_data).shape)
print("Shape of train_Y =", np.array(train_targets).shape)

#Normalize train_X, test_X, val_X

# Convolutional layers
def build_model():
  cnn_model = Sequential()
  for layer in vgg16_model.layers:
    cnn_model.add(layer)

  for layer in cnn_model.layers:
    layer.trainable = False

  cnn_model.add(Flatten())
  cnn_model.add(Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(64, kernel_regularizer = regularizers.l2(0.01), activation='elu'))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(3, activation = 'softmax'))

  return cnn_model

all_loss_hist = []
all_acc_hist = []
all_valloss_hist = []
all_valacc_hist = []

for i in range(k):
  print("Processing Fold #:", i)

  val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]

  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],
       train_data[(i+1) * num_val_samples:]], axis = 0
  )

  partial_train_targets = np.concatenate(
      [train_targets[:i * num_val_samples],
       train_targets[(i+1) * num_val_samples:]], axis = 0
  )

  cnn_model = build_model()

  cnn_model.compile(optimizer = 'nadam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

  # Train the model with the training data, set epochs and batch size
  history = cnn_model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 15, validation_data=(val_data, val_targets), verbose = 0)

  cnn_model.evaluate(test_X, test_Y)

  loss_hist = history.history['loss']
  acc_hist = history.history['accuracy']
  val_loss_hist = history.history['val_loss']
  val_acc_hist = history.history['val_accuracy']

  all_loss_hist.append(loss_hist)
  all_acc_hist.append(acc_hist)
  all_valloss_hist.append(val_loss_hist)
  all_valacc_hist.append(val_acc_hist)

avg_val_loss = [np.mean([x[i] for x in all_valloss_hist]) for i in range(num_epochs)]
avg_val_acc = [np.mean([x[i] for x in all_valacc_hist]) for i in range(num_epochs)]
avg_acc = [np.mean([x[i] for x in all_acc_hist]) for i in range(num_epochs)]
avg_loss = [np.mean([x[i] for x in all_loss_hist]) for i in range(num_epochs)]

#Plotting training and validation loss
import matplotlib.pyplot as plt

print(history_dict.keys())

epochs = range(1, len(avg_loss) + 1)

plt.plot(epochs, avg_loss, 'b', label = 'Training loss')
plt.plot(epochs, avg_val_loss, 'r', label = 'Validation loss')
plt.title("Training and Validation Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
epochs = range(1, len(avg_acc) + 1)

plt.plot(epochs, avg_acc, 'b', label = 'Training accuracy')
plt.plot(epochs, avg_val_acc, 'r', label = 'Validation accuracy')
plt.title("Training and Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()